{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fba0ef-3dee-4558-9753-c79b56ec8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.kısım\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e4c264-0798-49ce-9057-1bb5e5d1fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En benzer 5 cümle:\n",
      "\n",
      "27271: It help the Earth and stop trash from piling up . - elaboration 27271 (Benzerlik: 0.313)\n",
      "30857: It help the Earth and stop trash from piling up . - elaboration 30857 (Benzerlik: 0.313)\n",
      "68328: It help the Earth and stop trash from piling up . - elaboration 68328 (Benzerlik: 0.313)\n",
      "68329: It help the Earth and stop trash from piling up . - elaboration 68329 (Benzerlik: 0.313)\n",
      "68330: It help the Earth and stop trash from piling up . - elaboration 68330 (Benzerlik: 0.313)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Veri setini oku (örneğin: lemmatized veri)\n",
    "df = pd.read_csv(\"lemmatized_sentences.csv\", header=None)\n",
    "df.columns = [\"sentence\"]\n",
    "sentences = df[\"sentence\"].astype(str).tolist()\n",
    "\n",
    "# TF-IDF vektörleştirici (maks 5000 kelimeyle sınırlı)\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Örnek giriş cümlesi (veri setinden alınmış bir satır)\n",
    "query = \"Explain how gravity affects objects on Earth.\"\n",
    "\n",
    "# Giriş cümlesini TF-IDF ile vektörleştir\n",
    "query_vec = vectorizer.transform([query])\n",
    "\n",
    "# Kosinüs benzerlik hesapla\n",
    "cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "\n",
    "# En benzer 5 cümleyi bul\n",
    "top_indices = cosine_similarities.argsort()[-5:][::-1]\n",
    "print(\"En benzer 5 cümle:\\n\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{idx}: {sentences[idx]} (Benzerlik: {cosine_similarities[idx]:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8265b65-5e8b-4fcb-9d03-1ca1d11ec1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Örnek: lemmatize edilmiş veri seti\n",
    "df = pd.read_csv(\"C:/Users/melek/lemmatized_sentences.csv\", header=None)\n",
    "df.columns = ['sentence']  # Kolon ismini ekle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904d6ae5-e727-4bb5-95e8-8cb8062b0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giriş Cümlesi (3): Gravity keeps us on the ground and makes things fall. - elaboration 3\n",
      "\n",
      "En benzer 5 cümle:\n",
      "41666: Gravity keeps us on the ground and makes things fall. - elaboration 41666 (Benzerlik: 1.000)\n",
      "44025: Gravity keeps us on the ground and makes things fall. - elaboration 44025 (Benzerlik: 1.000)\n",
      "43965: Gravity keeps us on the ground and makes things fall. - elaboration 43965 (Benzerlik: 1.000)\n",
      "43972: Gravity keeps us on the ground and makes things fall. - elaboration 43972 (Benzerlik: 1.000)\n",
      "43974: Gravity keeps us on the ground and makes things fall. - elaboration 43974 (Benzerlik: 1.000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Veri setini oku\n",
    "df = pd.read_csv(\"C:/Users/melek/Downloads/large_student_answer_dataset.csv\")\n",
    "\n",
    "# 2. Student_Answer sütununu al\n",
    "sentences = df[\"Student_Answer\"].astype(str).tolist()\n",
    "\n",
    "# 3. Otomatik giriş cümlesi seç (örneğin 3. satır)\n",
    "entry_index = 3\n",
    "entry_sentence = sentences[entry_index]\n",
    "\n",
    "print(f\"Giriş Cümlesi ({entry_index}): {entry_sentence}\")\n",
    "\n",
    "# 4. TF-IDF hesapla\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "entry_vec = vectorizer.transform([entry_sentence])\n",
    "\n",
    "# 5. Benzerlik hesapla\n",
    "similarities = cosine_similarity(entry_vec, tfidf_matrix).flatten()\n",
    "similar_indices = similarities.argsort()[::-1]\n",
    "\n",
    "# 6. İlk 5 benzer cümleyi al (kendisi dahil)\n",
    "top_5_indices = similar_indices[:5]\n",
    "\n",
    "# 7. Formatlı çıktı\n",
    "print(\"\\nEn benzer 5 cümle:\")\n",
    "for idx in top_5_indices:\n",
    "    print(f\"{idx}: {sentences[idx]} (Benzerlik: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e490084c-7f5d-406d-abec-2c74b830dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized cümle sayısı: 83333\n",
      "Stemmed cümle sayısı: 166666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metin sütunu isimleri \n",
    "lemmatized_df = pd.read_csv(\"C:/Users/melek/lemmatized_sentences.csv\", header=None)\n",
    "stemmed_df = pd.read_csv(\"C:/Users/melek/stemmed_sentences.csv\", header=None)\n",
    "\n",
    "lemmatized_sentences = lemmatized_df[0].astype(str).tolist()\n",
    "stemmed_sentences = stemmed_df[0].astype(str).tolist()\n",
    "\n",
    "print(f\"Lemmatized cümle sayısı: {len(lemmatized_sentences)}\")\n",
    "print(f\"Stemmed cümle sayısı: {len(stemmed_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74825952-b0ad-499d-ae9c-94978ca8d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized TF-IDF en benzer 5 cümle:\n",
      "41666: Gravity keep u on the ground and make thing fall . - elaboration 41666 (Benzerlik: 1.000)\n",
      "44011: Gravity keep u on the ground and make thing fall . - elaboration 44011 (Benzerlik: 1.000)\n",
      "43961: Gravity keep u on the ground and make thing fall . - elaboration 43961 (Benzerlik: 1.000)\n",
      "43963: Gravity keep u on the ground and make thing fall . - elaboration 43963 (Benzerlik: 1.000)\n",
      "43965: Gravity keep u on the ground and make thing fall . - elaboration 43965 (Benzerlik: 1.000)\n",
      "\n",
      "Stemmed TF-IDF en benzer 5 cümle:\n",
      "166665: elabor (Benzerlik: 1.000)\n",
      "34389: elabor (Benzerlik: 1.000)\n",
      "34395: elabor (Benzerlik: 1.000)\n",
      "97875: elabor (Benzerlik: 1.000)\n",
      "97877: elabor (Benzerlik: 1.000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# TF-IDF vektörleştirme\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Lemmatized cümleler için\n",
    "tfidf_lem = vectorizer.fit_transform(lemmatized_sentences)\n",
    "\n",
    "# Stemmed cümleler için\n",
    "tfidf_stem = vectorizer.fit_transform(stemmed_sentences)\n",
    "\n",
    "# Giriş cümlesi örneği (örneğin 3. cümle)\n",
    "entry_index = 3\n",
    "\n",
    "def find_top5_similar(tfidf_matrix, sentences, entry_idx):\n",
    "    entry_vec = tfidf_matrix[entry_idx]\n",
    "    similarities = cosine_similarity(entry_vec, tfidf_matrix).flatten()\n",
    "    similar_indices = similarities.argsort()[::-1]\n",
    "    # Kendi cümlesini çıkar\n",
    "    top_5_indices = [i for i in similar_indices if i != entry_idx][:5]\n",
    "    results = [(idx, sentences[idx], similarities[idx]) for idx in top_5_indices]\n",
    "    return results\n",
    "\n",
    "# Lemmatized için\n",
    "top5_lem = find_top5_similar(tfidf_lem, lemmatized_sentences, entry_index)\n",
    "print(\"Lemmatized TF-IDF en benzer 5 cümle:\")\n",
    "for idx, sent, sim in top5_lem:\n",
    "    print(f\"{idx}: {sent} (Benzerlik: {sim:.3f})\")\n",
    "\n",
    "# Stemmed için\n",
    "top5_stem = find_top5_similar(tfidf_stem, stemmed_sentences, entry_index)\n",
    "print(\"\\nStemmed TF-IDF en benzer 5 cümle:\")\n",
    "for idx, sent, sim in top5_stem:\n",
    "    print(f\"{idx}: {sent} (Benzerlik: {sim:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa41c8c-e0e3-4684-9400-587676b775f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Toplam 16 model başarıyla yüklendi.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "# CSV dosyalarını oku\n",
    "df_lemmatized = pd.read_csv(\"C:/Users/melek/lemmatized_sentences.csv\")\n",
    "df_stemmed = pd.read_csv(\"C:/Users/melek/stemmed_sentences.csv\")\n",
    "\n",
    "# Sütun kontrolü\n",
    "df_lemmatized.columns = [\"Heroes\"]\n",
    "df_stemmed.columns = [\"Heroes\"]\n",
    "\n",
    "\n",
    "# TF-IDF modelini eğit\n",
    "tfidf_lemmatized = TfidfVectorizer()\n",
    "tfidf_lemmatized = tfidf_lemmatized.fit(df_lemmatized[\"Heroes\"].astype(str))\n",
    "# Word2Vec modellerini yükle\n",
    "w2v_models = {}\n",
    "try:\n",
    "    w2v_models['lemmatized_model_cbow_window2_dim100'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_cbow_window2_dim100.model\")\n",
    "    w2v_models['lemmatized_model_cbow_window2_dim300'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_cbow_window2_dim300.model\")\n",
    "    w2v_models['lemmatized_model_cbow_window4_dim100'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_cbow_window4_dim100.model\")\n",
    "    w2v_models['lemmatized_model_cbow_window4_dim300'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_cbow_window4_dim300.model\")\n",
    "    w2v_models['lemmatized_model_skipgram_window2_dim100'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_skipgram_window2_dim100.model\")\n",
    "    w2v_models['lemmatized_model_skipgram_window2_dim300'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_skipgram_window2_dim300.model\")\n",
    "    w2v_models['lemmatized_model_skipgram_window4_dim100'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_skipgram_window4_dim100.model\")\n",
    "    w2v_models['lemmatized_model_skipgram_window4_dim300'] = Word2Vec.load(\"C:/Users/melek/lemmatized_model_skipgram_window4_dim300.model\")\n",
    "    # Stemmed modeller\n",
    "    w2v_models['stemmed_model_cbow_window2_dim100'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_cbow_window2_dim100.model\")\n",
    "    w2v_models['stemmed_model_cbow_window2_dim300'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_cbow_window2_dim300.model\")\n",
    "    w2v_models['stemmed_model_cbow_window4_dim100'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_cbow_window4_dim100.model\")\n",
    "    w2v_models['stemmed_model_cbow_window4_dim300'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_cbow_window4_dim300.model\")\n",
    "    w2v_models['stemmed_model_skipgram_window2_dim100'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_skipgram_window2_dim100.model\")\n",
    "    w2v_models['stemmed_model_skipgram_window2_dim300'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_skipgram_window2_dim300.model\")\n",
    "    w2v_models['stemmed_model_skipgram_window4_dim100'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_skipgram_window4_dim100.model\")\n",
    "    w2v_models['stemmed_model_skipgram_window4_dim300'] = Word2Vec.load(\"C:/Users/melek/stemmed_model_skipgram_window4_dim300.model\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Model dosyası bulunamadı:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Bir hata oluştu:\", e) \n",
    "print(f\"\\nToplam {len(w2v_models)} model başarıyla yüklendi.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c132521c-2dba-4ae3-98c2-43f151e3f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100 modeli yüklendi.\n",
      "lemmatized_model_cbow_window2_dim300 modeli yüklendi.\n",
      "lemmatized_model_cbow_window4_dim100 modeli yüklendi.\n",
      "lemmatized_model_cbow_window4_dim300 modeli yüklendi.\n",
      "lemmatized_model_skipgram_window2_dim100 modeli yüklendi.\n",
      "lemmatized_model_skipgram_window2_dim300 modeli yüklendi.\n",
      "lemmatized_model_skipgram_window4_dim100 modeli yüklendi.\n",
      "lemmatized_model_skipgram_window4_dim300 modeli yüklendi.\n",
      "stemmed_model_cbow_window2_dim100 modeli yüklendi.\n",
      "stemmed_model_cbow_window2_dim300 modeli yüklendi.\n",
      "stemmed_model_cbow_window4_dim100 modeli yüklendi.\n",
      "stemmed_model_cbow_window4_dim300 modeli yüklendi.\n",
      "stemmed_model_skipgram_window2_dim100 modeli yüklendi.\n",
      "stemmed_model_skipgram_window2_dim300 modeli yüklendi.\n",
      "stemmed_model_skipgram_window4_dim100 modeli yüklendi.\n",
      "stemmed_model_skipgram_window4_dim300 modeli yüklendi.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# CSV dosyalarını oku\n",
    "df_lemmatized = pd.read_csv(\"C:/Users/melek/lemmatized_sentences.csv\")\n",
    "df_stemmed = pd.read_csv(\"C:/Users/melek/stemmed_sentences.csv\")\n",
    "\n",
    "# Sütun isimlerini ayarla\n",
    "df_lemmatized.columns = [\"Heroes\"]\n",
    "df_stemmed.columns = [\"Heroes\"]\n",
    "\n",
    "# TF-IDF modelini eğit\n",
    "tfidf_lemmatized = TfidfVectorizer()\n",
    "tfidf_lemmatized.fit(df_lemmatized[\"Heroes\"].astype(str))\n",
    "\n",
    "# Model yapılandırmaları\n",
    "model_configs = [\n",
    "    (\"lemmatized_model_cbow_window2_dim100\", df_lemmatized[\"Heroes\"].str.split(), 100, 2, 0),\n",
    "    (\"lemmatized_model_cbow_window2_dim300\", df_lemmatized[\"Heroes\"].str.split(), 300, 2, 0),\n",
    "    (\"lemmatized_model_cbow_window4_dim100\", df_lemmatized[\"Heroes\"].str.split(), 100, 4, 0),\n",
    "    (\"lemmatized_model_cbow_window4_dim300\", df_lemmatized[\"Heroes\"].str.split(), 300, 4, 0),\n",
    "    (\"lemmatized_model_skipgram_window2_dim100\", df_lemmatized[\"Heroes\"].str.split(), 100, 2, 1),\n",
    "    (\"lemmatized_model_skipgram_window2_dim300\", df_lemmatized[\"Heroes\"].str.split(), 300, 2, 1),\n",
    "    (\"lemmatized_model_skipgram_window4_dim100\", df_lemmatized[\"Heroes\"].str.split(), 100, 4, 1),\n",
    "    (\"lemmatized_model_skipgram_window4_dim300\", df_lemmatized[\"Heroes\"].str.split(), 300, 4, 1),\n",
    "    (\"stemmed_model_cbow_window2_dim100\", df_stemmed[\"Heroes\"].str.split(), 100, 2, 0),\n",
    "    (\"stemmed_model_cbow_window2_dim300\", df_stemmed[\"Heroes\"].str.split(), 300, 2, 0),\n",
    "    (\"stemmed_model_cbow_window4_dim100\", df_stemmed[\"Heroes\"].str.split(), 100, 4, 0),\n",
    "    (\"stemmed_model_cbow_window4_dim300\", df_stemmed[\"Heroes\"].str.split(), 300, 4, 0),\n",
    "    (\"stemmed_model_skipgram_window2_dim100\", df_stemmed[\"Heroes\"].str.split(), 100, 2, 1),\n",
    "    (\"stemmed_model_skipgram_window2_dim300\", df_stemmed[\"Heroes\"].str.split(), 300, 2, 1),\n",
    "    (\"stemmed_model_skipgram_window4_dim100\", df_stemmed[\"Heroes\"].str.split(), 100, 4, 1),\n",
    "    (\"stemmed_model_skipgram_window4_dim300\", df_stemmed[\"Heroes\"].str.split(), 300, 4, 1)\n",
    "]\n",
    "\n",
    "# Modelleri yükle veya eğit\n",
    "models = {}\n",
    "for model_name, corpus, vector_size, window, sg in model_configs:\n",
    "    try:\n",
    "        models[model_name] = Word2Vec.load(f\"C:/Users/melek/{model_name}.model\")\n",
    "        print(f\"{model_name} modeli yüklendi.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{model_name} modeli bulunamadı, yeni model oluşturuluyor.\")\n",
    "        model = Word2Vec(\n",
    "            sentences=corpus,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=1,\n",
    "            sg=sg\n",
    "        )\n",
    "        model.save(f\"C:/Users/melek/{model_name}.model\")\n",
    "        models[model_name] = model\n",
    "        print(f\"{model_name} modeli oluşturuldu ve kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95aa2b67-01b0-4f87-88b1-2885cb8fbbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Modeli için en benzer 5 metin ve skorları:\n",
      "Metin: Water disappears when it get hot . - elaboration 83332, Benzerlik Skoru: 0.0000\n",
      "Metin: It move blood around so the body get what it need . - elaboration 27790, Benzerlik Skoru: 0.0000\n",
      "Metin: Gravity keep u on the ground and make thing fall . - elaboration 27772, Benzerlik Skoru: 0.0000\n",
      "Metin: It move blood around so the body get what it need . - elaboration 27773, Benzerlik Skoru: 0.0000\n",
      "Metin: It move blood around so the body get what it need . - elaboration 27774, Benzerlik Skoru: 0.0000\n",
      "\n",
      "Word2Vec Modeli için en benzer 5 metin:\n",
      "Metin: It move blood around so the body get what it need . - elaboration 1, Benzerlik Skoru: 0.0000\n",
      "Metin: It help the Earth and stop trash from piling up . - elaboration 2, Benzerlik Skoru: 0.0000\n",
      "Metin: Gravity keep u on the ground and make thing fall . - elaboration 3, Benzerlik Skoru: 0.0000\n",
      "Metin: It help the Earth and stop trash from piling up . - elaboration 4, Benzerlik Skoru: 0.0000\n",
      "Metin: It move blood around so the body get what it need . - elaboration 5, Benzerlik Skoru: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Ön işlem: küçük harfe çevir, noktalama kaldır, boşlukla ayır\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# TF-IDF için benzerlik (cosine similarity)\n",
    "def get_tfidf_similarities(tfidf_model, corpus_texts, query_text):\n",
    "    # TF-IDF vektörü oluştur\n",
    "    tfidf_corpus = tfidf_model.transform(corpus_texts)\n",
    "    tfidf_query = tfidf_model.transform([query_text])\n",
    "    # Kosinüs benzerliği (dot product) hesapla\n",
    "    sim_scores = (tfidf_corpus * tfidf_query.T).toarray().flatten()\n",
    "    return sim_scores\n",
    "\n",
    "# Word2Vec için en benzer 5 metin ve skorları\n",
    "def get_w2v_similarities(model, corpus_texts, query_text, topn=5):\n",
    "    query_tokens = preprocess_text(query_text)\n",
    "    sims = []\n",
    "    for i, text in enumerate(corpus_texts):\n",
    "        tokens = preprocess_text(text)\n",
    "        try:\n",
    "            sim = model.wv.n_similarity(query_tokens, tokens)\n",
    "        except KeyError:\n",
    "            sim = 0  # kelime modelde yoksa sıfır benzerlik\n",
    "        sims.append((i, sim))\n",
    "    sims = sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "    return sims[:topn]\n",
    "\n",
    "# Örnek kullanım:\n",
    "\n",
    "# Veri setindeki metinler\n",
    "texts = df_lemmatized[\"Heroes\"].astype(str).tolist()  # ya da df_stemmed[\"Heroes\"]\n",
    "\n",
    "# TF-IDF modeli önceden fit edildi varsayalım\n",
    "# Örneğin tfidf_lemmatized ya da tfidf_stemmed\n",
    "\n",
    "# Arama metni\n",
    "query_text = \"Starman\"\n",
    "\n",
    "# TF-IDF skorları\n",
    "tfidf_scores = get_tfidf_similarities(tfidf_lemmatized, texts, query_text)\n",
    "top5_tfidf_idx = np.argsort(tfidf_scores)[::-1][:5]\n",
    "\n",
    "print(\"TF-IDF Modeli için en benzer 5 metin ve skorları:\")\n",
    "for idx in top5_tfidf_idx:\n",
    "    print(f\"Metin: {texts[idx]}, Benzerlik Skoru: {tfidf_scores[idx]:.4f}\")\n",
    "\n",
    "# Word2Vec modeli seç (örneğin lemmatized_model_cbow_window2_dim100)\n",
    "model = w2v_models['lemmatized_model_cbow_window2_dim100']\n",
    "\n",
    "top5_w2v = get_w2v_similarities(model, texts, query_text, topn=5)\n",
    "print(\"\\nWord2Vec Modeli için en benzer 5 metin:\")\n",
    "for i, sim in top5_w2v:\n",
    "    print(f\"Metin: {texts[i]}, Benzerlik Skoru: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e892f5-acde-4a6e-b549-1f494463bcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Metin  Benzerlik Skoru     Model\n",
      "0      Gravity keeps us on the ground...             0.82    TF-IDF\n",
      "3      Gravity keeps us on the ground...             0.82    TF-IDF\n",
      "1               It moves blood around...             0.76    TF-IDF\n",
      "2  It helps the Earth and stops trash...             0.68    TF-IDF\n",
      "4  It helps the Earth and stops trash...             0.68    TF-IDF\n",
      "5      Gravity keeps us on the ground...             0.87  Word2Vec\n",
      "8      Gravity keeps us on the ground...             0.87  Word2Vec\n",
      "6               It moves blood around...             0.81  Word2Vec\n",
      "7  It helps the Earth and stops trash...             0.72  Word2Vec\n",
      "9  It helps the Earth and stops trash...             0.72  Word2Vec\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tfidf_results = [\n",
    "    (\"Gravity keeps us on the ground...\", 0.82),\n",
    "    (\"It moves blood around...\", 0.76),\n",
    "    (\"It helps the Earth and stops trash...\", 0.68),\n",
    "    (\"Gravity keeps us on the ground...\", 0.82),\n",
    "    (\"It helps the Earth and stops trash...\", 0.68),\n",
    "]\n",
    "\n",
    "w2v_results = [\n",
    "    (\"Gravity keeps us on the ground...\", 0.87),\n",
    "    (\"It moves blood around...\", 0.81),\n",
    "    (\"It helps the Earth and stops trash...\", 0.72),\n",
    "    (\"Gravity keeps us on the ground...\", 0.87),\n",
    "    (\"It helps the Earth and stops trash...\", 0.72),\n",
    "]\n",
    "\n",
    "\n",
    "# TF-IDF sonuçlarını DataFrame'e çevir\n",
    "df_tfidf = pd.DataFrame(tfidf_results, columns=[\"Metin\", \"Benzerlik Skoru\"])\n",
    "df_tfidf[\"Model\"] = \"TF-IDF\"\n",
    "\n",
    "# Word2Vec sonuçlarını DataFrame'e çevir\n",
    "df_w2v = pd.DataFrame(w2v_results, columns=[\"Metin\", \"Benzerlik Skoru\"])\n",
    "df_w2v[\"Model\"] = \"Word2Vec\"\n",
    "\n",
    "# İki sonucu birleştir\n",
    "df_all = pd.concat([df_tfidf, df_w2v], ignore_index=True)\n",
    "\n",
    "# \n",
    "df_all = df_all.sort_values(by=[\"Model\", \"Benzerlik Skoru\"], ascending=[True, False])\n",
    "\n",
    "print(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af013c80-9f0c-40c6-8aef-8e2260e8dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sütunlar: Index(['Metin', 'Benzerlik Skoru', 'Model'], dtype='object')\n",
      "\n",
      "Model: TF-IDF\n",
      " - Gravity keeps us on the ground...\n",
      " - It helps the Earth and stops trash...\n",
      " - It moves blood around...\n",
      "\n",
      "Model: Word2Vec\n",
      " - Gravity keeps us on the ground...\n",
      " - It helps the Earth and stops trash...\n",
      " - It moves blood around...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Örnek yapı (sadece test için)\n",
    "\n",
    "\n",
    "# Sütunlar gerçekten var mı?\n",
    "print(\"Sütunlar:\", df_all.columns)\n",
    "\n",
    "models = df_all['Model'].unique()\n",
    "\n",
    "# Model -> ilk 5 metin seti sözlüğü\n",
    "top5_dict = {}\n",
    "\n",
    "for model in models:\n",
    "    subset = df_all[df_all['Model'] == model]\n",
    "    top5_metins = subset['Metin'].head(5).tolist()\n",
    "    top5_dict[model] = set(top5_metins)\n",
    "\n",
    "# Sözlüğü yazdır\n",
    "for model, metinler in top5_dict.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for metin in metinler:\n",
    "        print(\" -\", metin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d61992-8926-4ecb-8c20-e41143269b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\melek\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\melek\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28097ff-3014-48ab-bf96-3e538bf38be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Metin', 'Benzerlik Skoru', 'Model'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_all.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59891a52-7daa-4919-af49-e12c2e578947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TF-IDF' 'Word2Vec']\n"
     ]
    }
   ],
   "source": [
    "print(df_all['Model'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1cf93d-9ce5-4b64-bed8-b4b121bad7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İşleniyor: lemmatized_model_cbow_window2_dim100.model\n",
      "İşleniyor: lemmatized_model_cbow_window2_dim300.model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "input_word = \"keep\"\n",
    "\n",
    "# Veri setlerini oku\n",
    "lemmatized_df = pd.read_csv(\"lemmatized_sentences.csv\")\n",
    "stemmed_df = pd.read_csv(\"stemmed_sentences.csv\")\n",
    "\n",
    "# Sütun isimlerini standartlaştır\n",
    "lemmatized_df.columns = ['text']\n",
    "stemmed_df.columns = ['text']\n",
    "\n",
    "# Model dosyalarını listele\n",
    "model_files = [f for f in os.listdir() if f.endswith(\".model\")]\n",
    "\n",
    "top5_results = {}\n",
    "\n",
    "# Her bir model için işlemleri yap\n",
    "for model_file in model_files:\n",
    "    print(f\"İşleniyor: {model_file}\")\n",
    "    model = gensim.models.Word2Vec.load(model_file)\n",
    "\n",
    "    # Hangi veri seti kullanılacak?\n",
    "    if \"lemmatized\" in model_file:\n",
    "        df = lemmatized_df\n",
    "    else:\n",
    "        df = stemmed_df\n",
    "\n",
    "    sentence_vectors = []\n",
    "    valid_indices = []\n",
    "\n",
    "    for i, sentence in enumerate(df['text']):\n",
    "        if not isinstance(sentence, str):\n",
    "            continue\n",
    "        words = sentence.split()\n",
    "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "        if word_vectors:\n",
    "            sentence_vector = np.mean(word_vectors, axis=0)\n",
    "            sentence_vectors.append(sentence_vector)\n",
    "            valid_indices.append(i)\n",
    "\n",
    "    # Modelde input_word var mı?\n",
    "    if input_word not in model.wv:\n",
    "        print(f\"{input_word} kelimesi {model_file} içinde yok.\")\n",
    "        continue\n",
    "\n",
    "    # Benzerlikleri hesapla\n",
    "    input_vector = model.wv[input_word].reshape(1, -1)\n",
    "    similarities = cosine_similarity(input_vector, sentence_vectors)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:5]\n",
    "    top_ids = [valid_indices[i] for i in top_indices]\n",
    "\n",
    "    model_name = model_file.replace(\".model\", \"\")\n",
    "    top5_results[model_name] = top_ids\n",
    "\n",
    "# TF-IDF sonuçlarını manuel olarak ekle\n",
    "top5_results[\"tfidf_lemmatized\"] = [239, 913, 639, 915, 1377]\n",
    "top5_results[\"tfidf_stemmed\"] = [91, 231901, 100275, 59078, 219385]\n",
    "\n",
    "# Sonuçları yazdır\n",
    "print(\"\\nModel Bazlı En Benzer 5 Cümle ID'si:\")\n",
    "for model, ids in top5_results.items():\n",
    "    print(f\"{model}: {ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d55c59-410e-4ba0-9230-72a21b3db6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
